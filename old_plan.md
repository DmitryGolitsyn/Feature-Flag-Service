Отлично. Ниже — подробный, линейный план «микрошагов» для учебного проекта на Go (API + БД + Kafka), ровно в том стиле, как работает сеньор-наставник: маленький шаг → зачем → критерий «готово». Мы уже прошли часть шагов — я отмечу их как «СДЕЛАНО», и будем двигаться дальше строго по очереди. После каждого шага ты присылаешь архив — я синхронизируюсь и даю следующий.

---

# Фаза 0. Базовая скелет-структура (HTTP, конфиг, middlewares)

1. (СДЕЛАНО) Инициализировать модуль (`go mod init`), создать `cmd/ffs`, `internal/`.

    * Зачем: зафиксировать модуль и точку входа.
    * Готово: `go run ./cmd/ffs` печатает приветствие.

2. (СДЕЛАНО) Минимальный HTTP-сервер (`chi`), `/healthz`.

    * Зачем: «дыхание» сервиса.
    * Готово: `curl /healthz` → 200.

3. (СДЕЛАНО) `/readyz` + флаг готовности.

    * Зачем: отделить liveness от readiness.
    * Готово: `/readyz` → 200, можно переключить на 503.

4. (СДЕЛАНО) Graceful shutdown по `SIGINT/SIGTERM`.

    * Зачем: корректное завершение.
    * Готово: при Ctrl+C — «shutting down… bye».

5. (СДЕЛАНО) Вынести HTTP-сервер в `internal/httpserver`.

    * Зачем: слои/композиция.
    * Готово: `cmd/ffs` — только «проводка».

6. (СДЕЛАНО) Middleware: Request-ID.

    * Зачем: трассировка запроса.
    * Готово: `X-Request-Id` в ответе/логах.

7. (СДЕЛАНО) Middleware: RealIP + User-Agent.

    * Зачем: клиентский контекст в логах.
    * Готово: лог содержит reqID/IP/UA.

8. (СДЕЛАНО) Конфиг `internal/config`: `PORT`, таймауты сервера.

    * Зачем: без хардкодов.
    * Готово: порт/таймауты читаются из ENV.

9. (СДЕЛАНО) Версионирование API: `/v1` + `POST /v1/ping`.

    * Зачем: скелет публичного API.
    * Готово: JSON `{ok:true, request_id:"…"}`.

10. (СДЕЛАНО) Единые JSON-ответы и ошибки (`respond.go`).

* Зачем: консистентность API.
* Готово: `JSON(...)` и `ErrorJSON(...)` используются в хендлерах.

---

# Фаза 1. Usecase-слой и типизация ошибок

11A) (СДЕЛАНО) `internal/app.Application`, первый usecase (Ping/Echo), HTTP вызывает usecase; проброс ошибок usecase → HTTP.

* Зачем: веб-слой тонкий, логика в usecase.
* Готово: `/v1/echo` вызывает `app.Echo.Do(...)`, ошибки маппятся.

11B) **Типизация ошибок usecase**: `app`-ошибки (+ обёртки), `errors.Is`, `error_mapper`.

* Зачем: предсказуемые статусы (400/403/404/500), обогащение контекстом.
* Сделать:

    * В `internal/app/errors.go` — базовые варры (`ErrInvalid`, `ErrForbidden`, `ErrNotFound`, `ErrInternal`).
    * (Если нужно) сделать `Wrap(err, op, meta)` с `fmt.Errorf("%w", err)` и хранением operation.
    * В `internal/httpserver/error_mapper.go` — `MapErrorToStatus(err)` (у тебя уже есть).
* Готово: любые usecase-ошибки маппятся стабильным образом.

11C) **DTO и валидация входящих данных** (микро-валидация вручную).

* Зачем: чистый контракт API.
* Сделать:

    * В `api_v1.go` у `handleEcho` — `Decode JSON → validate → usecase`.
    * На ошибки валидации → `ErrorJSON(400,"invalid_input")`.
* Готово: некорректный JSON/поля → 400.

---

# Фаза 2. Хранилище: Postgres (миграции, репозитории, транзакции)

12A) Подключение PG: `internal/infra/postgres` (pgx pool, `PG_DSN` из ENV).

* Зачем: реальная БД.
* Готово: на старте пинингуем PG; `/readyz` зависит от коннекта.

12B) Миграции (минимально): `migrations/001_init.sql` + утилита/скрипт запуска.

* Зачем: эволюция схемы.
* Готово: `make migrate` применяет миграции.

12C) Домашняя таблица (например, `echo_messages(id, message, created_at)`).

* Зачем: простая запись для первого репозитория.
* Готово: таблица создана миграцией.

12D) Репозиторий `internal/app/echo_repo.go` с интерфейсом `EchoSaver`.

* Зачем: DIP между usecase и БД.
* Готово: usecase принимает интерфейс, реализация в `infra`.

12E) Транзакции и контекст-таймауты (`WithTimeout(ctx, 3s)` на запрос).

* Зачем: контроль долговых операций.
* Готово: операции в транзакциях; таймауты у запросов.

12F) Обработка ошибок PG → usecase-ошибки (конфликты/нет данных).

* Зачем: корректный `MapErrorToStatus`.
* Готово: нарушенные ограничения → 400/409; пусто → 404.

---

# Фаза 3. Внешние интеграции: Kafka (producer/consumer)

13A) Producer: `internal/infra/kafka` (адреса из ENV).

* Зачем: отправка событий (Audit/Event-Driven).
* Готово: usecase после записи в БД публикует событие.

13B) Идемпотентность продьюсера (ключ/повторы, headers с `Request-ID`).

* Зачем: отсутствие дублей downstream.
* Готово: ключи сообщений, повтор — безопасен.

13C) Consumer (отдельная `cmd/ffs-worker`).

* Зачем: асинхронная обработка.
* Готово: отдельный бинарь, общий `Application`/конфиг.

13D) Graceful shutdown для consumer (commit offsets → stop).

* Зачем: не терять сообщения.
* Готово: Ctrl+C — завершение без потерь.

---

# Фаза 4. Обсервабилити: логи, метрики, трейсинг

14A) Структурный логгер (JSON) + уровни (info/error/debug).

* Зачем: машинно-читаемые логи.
* Готово: единый логгер в контексте запроса (вытаскивает reqID/IP/UA).

14B) Prometheus-метрики (`/metrics`): http_requests_total, duration_histogram, db_query_duration, kafka_produced_total.

* Зачем: видимость.
* Готово: базовые метрики по HTTP/DB/Kafka.

14C) Трейсинг OpenTelemetry (опционально): spans для HTTP, DB, Kafka.

* Зачем: распределённые трассы.
* Готово: локально видны трейсинг-спаны (OTLP).

---

# Фаза 5. Безопасность и прод-твёрдость

15A) JWT-аутентификация (middleware): парсинг, проверка подписей, контекст пользователя.

* Зачем: защищённые ручки.
* Готово: `/v1/*` защищается (или часть маршрутов).

15B) RBAC на уровне хендлеров (role → разрешения).

* Зачем: доступ по ролям.
* Готово: 403 при недостатке прав.

15C) Idempotency-Key (для POST) — защита от повторной отправки.

* Зачем: безопасные повторы со стороны клиента/прокси.
* Готово: повторный POST с тем же ключом возвращает предыдущий результат.

15D) Rate limiting (per IP / per user), простая in-memory версия.

* Зачем: защита от злоупотреблений.
* Готово: 429 при превышении лимита.

15E) Валидация схемы запросов (по необходимости — `jsonschema` или ручная).

* Зачем: защитная обшивка API.
* Готово: недопустимые поля/тип → 400.

---

# Фаза 6. Кеш/Redis и CQRS-штрихи (опционально)

16A) Подключить Redis (ENV), простой кэш на чтение.

* Зачем: ускорить «горячие» GET.
* Готово: метрики hit/miss.

16B) Инвалидирование кэша по событиям Kafka.

* Зачем: eventual consistency.
* Готово: запись → событие → consumer чистит кэш.

---

# Фаза 7. Тесты и локальная инфраструктура

17A) Юнит-тесты usecase (моки репозиториев).

* Зачем: проверка бизнес-логики без внешних зависимостей.
* Готово: `go test ./...` зелёный.

17B) Интеграционные тесты с реальным PG (testcontainers или Docker-compose).

* Зачем: проверка схемы/SQL/транзакций.
* Готово: `make test-int` поднимает PG и гоняет тесты.

17C) Контракт-тесты API (например, `httptest`).

* Зачем: стабильность контракта.
* Готово: проверка кодов/тел/заголовков.

---

# Фаза 8. Сборка, запуск, профилирование, нагрузка

18A) Dockerfile для `cmd/ffs` и `cmd/ffs-worker` (multi-stage).

* Зачем: деплой.
* Готово: `docker build`/`docker compose up` работает.

18B) `pprof` эндпоинты (локально) + чек-лист по профилированию.

* Зачем: CPU/heap профили.
* Готово: профили собираются и читаются.

18C) k6 сценарий (smoke + RPS + latency SLO).

* Зачем: подтверждение производительности.
* Готово: отчёт latency p95/p99.

18D) Chaos (опционально): Toxiproxy для PG/Kafka (добавить задержки/дропы).

* Зачем: устойчивость.
* Готово: сценарии деградации, сервис ведёт себя корректно.

---

# Фаза 9. Клиентский SDK (минимально)

19A) Go SDK-клиент для `/v1/*` (with Request-ID, Idempotency-Key, Retry).

* Зачем: потребление API другими сервисами.
* Готово: `ffsclient.Echo(ctx, msg)` работает.

19B) (Опционально) Python/TS SDK.

* Зачем: многоплатформенность.
* Готово: базовые вызовы API.

---

# Правила работы дальше

* Держимся **микрошагов**. Каждый шаг: «что/зачем/готово».
* После каждого шага — присылаешь архив проекта. Я синхронизируюсь и даю следующий.
* Если где-то я «убежал вперёд» — стопаемся и возвращаемся к последнему зафиксированному шагу.

---

## Где мы сейчас и следующий шаг

По твоему коду в архиве: у тебя уже есть `Application`, usecase Echo, `error_mapper`, `respond`, `api_v1` с `ping/echo`, middlewares, конфиг — т.е. **Фаза 1 завершена**, мы готовы двигаться к **Фазе 2 (Postgres)**.

### Предлагаю следующий конкретный шаг — **12A**:

**Подключение PG (pgxpool) + проверка подключения + влияние на `/readyz`**.

* Что сделать:

    1. `internal/infra/postgres`: `NewPool(ctx, dsn) (*pgxpool.Pool, error)` и `Ping(ctx)`.
    2. ENV: `PG_DSN` (с дефолтом на локальный `postgres://...`).
    3. В `Application` добавить ссылку на репозитории позже; сейчас просто храним pool в `Server`/или `Application`.
    4. В `server.New(...)` после старта — фоновой горутиной пинговать PG и менять `ready` (1/0).
* Готово: если PG недоступен — `/readyz` → 503; подключаем PG — `/readyz` → 200.

Если ок — пиши «Готово 12A начну» и скидывай архив после выполнения.


План шагов (дальше от вашего текущего состояния)
11B (сделано/в процессе): типизация ошибок usecase, errors.Is, маппинг в HTTP-статусы.
12A. Домейн “Feature Flag” (in-memory):
ввести доменные типы Flag, Rule, Actor, валидацию;
интерфейс репозитория FlagRepository в usecase-слое;
in-memory адаптер;
usecase FlagUpsert, FlagEvaluate;
HTTP: POST /v1/flags (upsert), POST /v1/evaluate.
12B. Контракты DTO + валидация: строгие JSON-форматы, ошибки в единый формат, idempotency-key.
13A. БД (Postgres) фундамент:
модуль internal/infra/pg (подключение, миграции, migrate cmd);
репозиторий FlagRepository на SQL (вторая реализация).
13B. Транзакции и контекст: транзакционный boundary в usecase, context сквозной.
14A. Кэш Redis: read-through/ write-through для флагов, инвалидация.
14B. Метрики и алерты: HTTP p95, кэш hit/miss, DB latency, алерты на 5xx.
15A. Аутентификация/авторизация (минимум): JWT bearer, роль admin для /v1/flags.
15B. Аудит-лог: кто, что изменил (лучше Kafka “flag-events” → возможен CDC в будущем).
16A. Kafka (по требованию): продюсер “flag-updated”; консюмер “cache-invalidate”.
17A. Тесты: table-tests для usecase, http-handler tests (httptest), in-memory repo замены.
18A. CI: go vet, staticcheck, golangci-lint, go test -race.
19A. Конфиги: единый internal/config (HTTP, PG, Redis, JWT…), профили dev/stage/prod.
20A. Обзорка: логгер JSON, request-scoped поля (req_id, ip, ua), sampling.
21+. Харднинг: rate-limit, cors, pagination, SLO/SLI, pprof, readiness глубже.